%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012
%%%%
%%%% This book is distributed under a Creative Commons Attribution 3.0
%%%% Unported (CC BY 3.0) license and made possible by funding from
%%%% The Saylor Foundation \url{http://www.saylor.org}.
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Monte Carlo simulation is a broad term for methods that use random
numbers and statistical sampling to solve problems, rather than exact
modeling. From the nature of this sampling, the result will have some
uncertainty, but the statistical `law of large numbers' will ensure
that the uncertainty goes down as the number of samples grows. The
statistical law that underlies this is as follows: if $N$ independent
observations are made of a quantity with standard deviation~$\sigma$,
then the standard deviation of the mean is~$\sigma/\sqrt N$. This
means that more observations will lead to more accuracy; what makes
Monte Carlo methods interesting is that this gain in accuracy is not
related to dimensionality of the original problem.

Monte Carlo techniques are of course natural candidatates for simulating
phenomena that are statistical in nature, such as radioactive decay,
or Brownian motion.
Other problems where Monte Carlo
simulation is attractive are outside the realm of scientific
computing. For instance, the Black-Scholes model for stock option
pricing~\cite{BlackScholes} uses Monte Carlo simulation.

Some problems that you have seen before, such as solving a linear
system of equations, can be tackled with Monte Carlo
techniques. However, this is not a typical application. Below we will
discuss two applications where exact methods
would take far too much time to compute and where statistical sampling
can quickly give a reasonably accurate answer.

An important tool for statistical sampling is random number generator.
We start by briefly discussing the problems in generating random
numbers, especially in parallel.

\Level 0 {Parallel Random Number Generation}
\label{sec:parallel-random}
\index{random numbers|(}

Random numbers are often used in simulations as some examples below
will show. True random numbers are very hard to obtain: they could be
generated by measuring quantum processes such as radioactive
particles, but this is very cumbersome. Instead, we use
\emph{pseudo-random numbers}\index{pseudo-random numbers|see{random
    numbers}}. This means that we use a deterministic mathematical
process, that is sufficiently irregular that for practical purposes no
order can be found in it.

An easy way to generate random numbers (we leave off the `pseudo'
qualification) is to use linear congruential generators (for all you
ever need to know about random numbers, see Knuth~\cite{Knuth:vol2}),
recurrences of the form
\[ x_{k+1} = (ax_k+b) \mod m. \]
This sequence is periodic, since it consists of nonnegative integers at most
$m-1$, and with period $m$ under certain conditions. A
typical period is $2^{31}$. The starting point $x_0$ of the series is
known as the `seed'. Software for random numbers often lets you
specify the seed. To get reproducible results you would run your
program with the same seed multiple times; to get random behaviour
over multiple runs of your program you could for instance derive the
seed from clock and calendar functions.

Random number generation is problematic in parallel. To see this,
consider a parallel process that uses a random number generator on
each subprocess, and
consider a single processor emulating the parallel process. Now this
single process in effect has a random number generator that consists
of interleaving the parallel generator results. This means that, if we
use the same generator in all parallel processes, the effective
generator over the whole process will produce stretches of identical
values.

We can solve this problem by having a central task that supplies the
random values for all processes, but this introduces a
bottleneck. A~better solution is to set up independent generators with
parameter choices that guarantee statistical randomness. This is not
simple. For instance, if two sequences $x^{(1)}_i,x^{(2)}_i$ have the
same values of $a,b,m$, and their starting points are close together,
the sequences will be strongly correlated. Less trivial examples of
correlation exist.

Various techniques for random number generation exist, such as using
two sequences, where one generates the starting points for the other
sequence, which is the one actually used in the simulation. Software
for parallel random number generator can be found at
\url{http://sprng.cs.fsu.edu/}~\cite{Mascagni:SPRNG}.

\index{random numbers|)}

\Level 0 {Examples}

\Level 1 {Integration by statistics}

Let's start with something simpler than integration: measuring an
area. Suppose you have a pond of an irregular shape in your backyard,
and that the yard itself is rectangular with known dimensions. If you
would now throw pebbles into your yard so that they are equally likely
to land at any given spot, then the ratio of pebbles falling in the
pond to those falling outside equals the ratio of the areas. 

Less
fanciful and more mathematically, let $\Omega\in[0,1]^2$, and let 
a function $f(\bar x)$ describing
the boundary of~$\Omega$, that is
\[ 
\begin{cases}
  f(\bar x)<0&x\not\in\Omega\\
  f(\bar x)>0&x\in\Omega\\
\end{cases}
\]
Now take random points $\bar x_0,\bar x_1,\bar x_2\in[0,1]^2$, then we
can estimate the area of $\Omega$ by counting how often $f(\bar x_i)$
is positive or negative.

We can extend this idea to integration. The average value of a
function on an interval $(a,b)$ is defined as
\[ \langle f\rangle = \frac1{b-a}\int_a^bf(x)dx \]
On the other hand, we can also estimate the average as
\[ \langle f\rangle \approx \frac 1N\sum_{i=1}^nf(x_i) \]
if the points $x_i$ are reasonably distributed and the function $f$ is
not too wild. This leads us to
\[ \int_a^bf(x)dx  \approx (b-a) \frac 1N\sum_{i=1}^nf(x_i) \]
Statistical theory, that we will not go into,
tells us that the uncertainty $\sigma_I$ in the integral is related to
the standard deviation $\sigma_f$ by
\[ \sigma_I\sim \frac1{\sqrt N}\sigma_f \]
for normal distributions.

So far, Monte Carlo integration does not look much different from
classical integration. The difference appears when we go to higher
dimensions. In that case, for classical integration we would need $N$
points in each dimension, leading to $N^d$ points in $d$
dimensions. In the Monte Carlo method, on the other hand, the points
are taken at random from the $d$-dimensional space, and a much lower
number of points suffices.

Computationally, Monte Carlo methods are attractive since all function
evaluations can be performed in parallel.

\Level 1 {Monte Carlo simulation of the Ising model}
\label{sec:ising}
\index{Ising model|(}
\input chapters-public/ising
\index{Ising model|)}

