  In this exercise you will analyze the parallel sparse matrix-vector
  product for a hypothetical, but realistic, parallel machine.
  Let the machine parameters be characterized by (see
  section~\ref{sec:latencybandwidth}):
  \begin{itemize}
  \item {\it Latency:} $\alpha=1\mu s=10^{-6}s$.
  \item {\it Bandwidth:} $1Gb/s$ corresponds to $\beta=10^{-9}$.
  \item {\it Computation rate:} A per-core flops rate of $1G$flops
    means $\gamma=10^9$. This number may seem low, but note that the
    matrix-vector product has less reuse than the matrix-matrix
    product, which can achieve close to peak performance,
    and that the sparse matrix-vector product is even more
    bandwidth-bound.
  \end{itemize}
  We assume $10^4$ processors, and a five-point
  stencil matrix of size $N=25\cdot 10^{10}$. This means each
  processor stores $5\cdot 8\cdot N/p=10^9$ bytes. If the matrix comes
  from a problem on a square domain, this means the domain was size
  $n\times n$ where $n=\sqrt N=5\cdot 10^5$.

  Case 1. Rather than dividing the matrix, we divide the domain, and
  we do this first by horizontal slabs of size $n\times (n/p)$. Argue
  that the communication complexity is $2(\alpha+n\beta)$ and
  computation complexity is $10\cdot n\cdot (n/p)$. Show that the
  resulting computation outweighs the communication by a factor~250.

  Case 2. We divide the domain into patches of size $(n/\sqrt p)\times
  (n/\sqrt p)$. The memory and computation time are the same as
  before. Derive the communication time and show that it is better by
  a factor of~50.

  Argue that the first case does not weakly scale: under the
  assumption that $N/p$ is constant the speedup will go down.
  Argue that the second case does scale weakly.
