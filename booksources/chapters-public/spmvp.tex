%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012
%%%%
%%%% This book is distributed under a Creative Commons Attribution 3.0
%%%% Unported (CC BY 3.0) license and made possible by funding from
%%%% The Saylor Foundation \url{http://www.saylor.org}.
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In section~\ref{sec:sparse} you saw a first discussion of sparse
matrices, limited to use on a single processor. We will now go into
parallelism aspects.

The dense matrix-vector product, as you saw above,
required each processor to communicate with every other, and to have a
local buffer of essentially the size of the global vector. In the
sparse case, considerably less buffer space, as well as less
communication, is needed. Let us analyze this case. We will assume
that the matrix is distributed by block rows, where processor $p$ owns
the matrix rows with indices in some set~$I_p$.

The line $y_i=y_i+a_{ij}x_j$ now has to take into account that $a_{ij}$
can be zero. In particular, we need to consider that, for some pairs
$i\in I_p, j\not\in I_p$ no communication will be needed. Declaring
for each $i\in I_p$ a~sparsity pattern set
\[ S_{p;i}=\{j\colon j\not\in I_p, a_{ij}\not=0\} \]
our multiplication instruction becomes
\[ y_i\mathop{+=}a_{ij}x_j\qquad\hbox{if $j\in S_{p;i}$}. \]
If we want to avoid, as above, a flood of small messages, we combine
all communication into a single message per processor. Defining
\[ S_p = \cup_{i\in I_p} S_{p;i}, \]
the algorithm now becomes:
\begin{itemize}
\item Collect all necessary off-processor elements $x_j$ with $j\in
  S_p$ into one buffer;
\item Perform the matrix-vector product, reading all elements of~$x$
  from local storage.
\end{itemize}

This whole analysis of course also applies to dense matrices. This
becomes different if we consider where sparse matrices come from.  Let
us start with a simple case.

Recall figure~\ref{fig:laplacedomain}, which illustrated a discretized
boundary value problem on the simplest domain, a square, and let us
now parallelize it. We do this by partitioning the domain; each
processor gets the matrix rows corresponding to its subdomain.
Figure~\ref{fig:laplaceparallel}
\begin{figure}
  \begin{quote}
    \includegraphics[scale=.12]{graphics-public/laplaceparallel}
  \end{quote}
  \caption{A difference stencil applied to a two-dimensional square
    domain, distributed over processors. A cross-processor connection
    is indicated.}
  \label{fig:laplaceparallel}
\end{figure}
shows how this gives rise to connections between processors: the
elements $a_{ij}$ with $i\in I_p,j\not\in I_p$ are now the `legs' of
the stencil that reach beyond a processor boundary. The set of all
such~$j$, formally defined as 
\[ G = \{ j\not\in I_p \colon 
    \exists_{i\in I_p}\colon a_{ij}\not=0 \}
\]
is known as the \indexterm{ghost region} of a processor; see
figure~\ref{fig:ghost}.
\begin{figure}
  \includegraphics[scale=.25]{graphics-public/ghostpdf}
  \caption{The ghost region of a processor, induced by a stencil}
  \label{fig:ghost}
\end{figure}

\begin{exercise}
  Show that a one-dimensional partitioning of the domain leads to a
  partitioning of the matrix into block rows, but a two-dimensional
  partitioning of the domain does not. You can do this in the
  abstract, or you can illustrate it: take a $4\times4$ domain (giving
  a matrix of size~16), and partition it over 4 processors. The
  one-dimensional domain partitioning corresponds to giving each
  processor one line out of the domain, while the two-dimensional
  partitioning gives each processor a $2\times2$ subdomain. Draw the
  matrices for these two cases.
\end{exercise}

Our crucial observation is now that, for each processor, the number of
other processors it is involved with is strictly limited. This has
implications for the efficiency of the operation.

\Level 1 {Parallel efficiency of the sparse matrix-vector product}
\label{sec:par-spmvp}

In the case of the dense matrix-vector product
(section~\ref{sec:densescaling}), partitioning the matrix over the
processors by (block) rows did not lead to a scalable algorithm. Part
of the reason was the increase in the number of neighbours that each
processors needs to communicate with. Figure~\ref{fig:laplaceparallel}
shows that, for the matrix of a 5-five point stencil, this number is
limited to four.

\begin{exercise}
  Take a square domain and a partitioning of the variables of the
  processors as in figure~\ref{fig:laplaceparallel}.
  What is the maximum number of neighbours a
  processor needs to communication with for the box stencil in
  figure~\ref{fig:stencils}? In three space dimensions, what is the
  number of neighbours if a 7-point central difference stencil is
  used?
\end{exercise}

The observation that each processor communicates with just a few
neighbours stays intact if we go beyond square domains to more
complicated physical objects. If a processor receives a more or less
contiguous subdomain, the number of its neighbours will be
limited. This implies that even in complicated problems each processor
will only communicate with a small number of other processors. Compare
this to the dense case where each processor had to receive data from
\emph{every} other processor. It is obvious that the sparse case is
far more friendly to the interconnection network. (The fact that it
also more common for large systems may influence the choice of network
to install if you are about to buy a new parallel computer.)

For square domains, this argument can easily be made formal. Let the
unit domain $[0,1]^2$ be partitioned over $P$ processors in a $\sqrt
P\times \sqrt P$ grid. From figure~\ref{fig:laplaceparallel} we see
that every processor communicates with at most four neighbours. Let
the amount of work per processor be~$w$ and the communication time
with each neighbour~$c$. Then the time to perform the total work on a
single processor is $T_1=Pw$, and the parallel time is $T_P=w+4c$,
giving a speed up of
\[ S_P=Pw/(w+4c)=P/(1+4c/w)\approx P(1-4c/w). \]

\begin{exercise}
  Express $c$ and $w$ as functions of $N$ and~$P$, and show that the
  speedup is asymptotically optimal, under weak scaling of the problem.
\end{exercise}

\begin{exercise}
  In this exercise you will analyze the parallel sparse matrix-vector
  product for a hypothetical, but realistic, parallel machine.
  Let the machine parameters be characterized by (see
  section~\ref{sec:latencybandwidth}):
  \begin{itemize}
  \item {\it Latency:} $\alpha=1\mu s=10^{-6}s$.
  \item {\it Bandwidth:} $1Gb/s$ corresponds to $\beta=10^{-9}$.
  \item {\it Computation rate:} A per-core flops rate of $1G$flops
    means $\gamma=10^9$. This number may seem low, but note that the
    matrix-vector product has less reuse than the matrix-matrix
    product, which can achieve close to peak performance,
    and that the sparse matrix-vector product is even more
    bandwidth-bound.
  \end{itemize}
  We assume $10^4$ processors, and a five-point
  stencil matrix of size $N=25\cdot 10^{10}$. This means each
  processor stores $5\cdot 8\cdot N/p=10^9$ bytes. If the matrix comes
  from a problem on a square domain, this means the domain was size
  $n\times n$ where $n=\sqrt N=5\cdot 10^5$.

  Case 1. Rather than dividing the matrix, we divide the domain, and
  we do this first by horizontal slabs of size $n\times (n/p)$. Argue
  that the communication complexity is $2(\alpha+n\beta)$ and
  computation complexity is $10\cdot n\cdot (n/p)$. Show that the
  resulting computation outweighs the communication by a factor~250.

  Case 2. We divide the domain into patches of size $(n/\sqrt p)\times
  (n/\sqrt p)$. The memory and computation time are the same as
  before. Derive the communication time and show that it is better by
  a factor of~50. 

  Argue that the first case does not weakly scale: under the
  assumption that $N/p$ is constant the efficiency will go down. 
  (Show that speedup still goes up asymptotically as~$\sqrt p$.)
  Argue that the second case does scale weakly.
\end{exercise}

The argument that a processor will only connect with a few neighbours
is based on the nature of the scientific computations. It is true for
\ac{FDM} and \ac{FEM} methods.
In the case
of the \indexac{BEM}, any subdomain needs to communicate with
everything in a radius~$r$ around it. As the number of processors goes
up, the number of neighbours per processor will also go up.

\begin{exercise}
  Give a formal analysis of the speedup and efficiency of the \ac{BEM}
  algorithm. Assume again a unit amount of work~$w$ per processor and
  a time of communication~$c$ per neighbour. Since the notion of
  neighbour is now based on physical distance, not on graph
  properties, the number of neighbours will go up. Give
  $T_1,T_p,S_p,E_p$ for this case.
\end{exercise}

There are also cases where a sparse matrix needs to be handled
similarly to a dense matrix. For instance, \indexterm{Google}'s
\indexterm{PageRank} algorithm (see section~\ref{sec:pagerank}) has at its
heart the repeated operation $x\leftarrow Ax$ where $A$~is a sparse
matrix with $A_{ij}\not=0$ if web page~$j$ links to page~$i$; see
section~\ref{sec:pagerank}. This makes $A$ a very sparse matrix, with
no obvious structure, so every processor will most likely communicate
with almost every other.


\Level 1 {Memory behaviour of the sparse matrix-vector product}

In section~\ref{sec:mvp-opt} you saw an analysis of the sparse
matrix-vector product in the dense case, on a single processor. Some
of the analysis carries over immediately to the sparse case, such as
the fact that each matrix element is used only once and that the
performance is bound by the bandwidth between processor and memory.

Regarding reuse of the input and the output vector, if the matrix is
stored by rows, such as in \indexac{CRS} format (section~\ref{sec:crs}),
access to the output vector will be limited to one write per matrix
row.
On the other hand, the loop unrolling
trick for getting reuse of the input vector can not be applied
here. Code that combines two iterations is as follows:
\begin{verbatim}
for (i=0; i<M; i+=2) {
  s1 = s2 = 0;
  for (j) {
    s1 = s1 + a[i][j] * x[j];
    s2 = s2 + a[i+1][j] * x[j];
  }
  y[i] = s1; y[i+1] = s2;
}
\end{verbatim}
The problem here is that if $a_{ij}$ is nonzero, it is not guaranteed
that $a_{i+1,j}$ is nonzero. The irregularity of the sparsity pattern
makes optimizing the matrix-vector product hard. Modest improvements
are possible by identifying parts of the matrix that are small dense
blocks~\cite{ButtEijkLang:spmvp,DemEtAl:ieeeproc2004,oski}.

On a \indexac{GPU} the sparse matrix-vector product is also limited by
memory bandwidth. Programming is now harder because the \ac{GPU} has
to work in data parallel mode, with many active threads. 

An
interesting optimization becomes possible if we consider the context
in which the sparse matrix-vector product typically appears. The most
common use of this operation is in iterative solution methods for
linear systems (section~\ref{sec:iterative}), where it is applied with
the same matrix in possibly hundreds of iterations. Thus we could
consider leaving the matrix stored on the GPU and only copying the
input and output vectors for each product operation.


\Level 1 {The transpose product}
\label{sec:shared-crs-transpose}

In section~\ref{sec:crs} you saw that the code for both the regular
and the tranpose matrix-vector product are limited to loop orderings
where rows of the matrix are traversed. (In section~\ref{sec:locality}
you saw a discussion of computational effects of changes in loop
order; in this case we are limited to row traversal by the storage
format.) 

In this section we will briefly look at the parallel transpose
product. Equivalently to partitioninging the matrix by rows and
performing the transpose product, we look at a matrix stored and
partitioned by columns and perform the regular product.

The  algorithm for the product by columns can be given as:
\begin{quote}
    \begin{tabbing}
      $y\leftarrow 0$\\
      for \=$j$:\\
      \> for \=$i$:\\
      \>\> $y_i\leftarrow y_i+a_{ij}x_j$
    \end{tabbing}
\end{quote}
Both in shared and distributed memory we distribute outer iterations
over processors. The problem is then that each outer iteration updates
the whole output vector. This is a problem: with shared memory it
leads to multiple writes to locations in the output and in distributed
memory it requires communication that is as yet unclear.

One way to solve this would be to allocate a private output
vector~$y^{(p)}$ for each process:
\begin{quote}
    \begin{tabbing}
      $y^{(p)}\leftarrow 0$\\
      for \=$j\in$ the rows of processor $p$\\
      \> for \=all $i$:\\
      \>\> $y^{(p)}_i\leftarrow y^{(p)}_i+a_{ji}x_j$
    \end{tabbing}
\end{quote}
after which we sum $y\leftarrow\sum_py^{(p)}$.

\Level 1 {Setup of the sparse matrix-vector product}

While in the square
domain case it was easy for a processor to decide which are its
neighbours, in the sparse case this is not so simple. The
straightforward way to proceed is to have a preprocessing stage:
\begin{itemize}
\item Each processor makes an inventory of what non-local indices it
  needs; assuming that each processor knows what range of indices each
  other processor owns, it then decides which indices to get from what
  neighbours.
\item Each processor sends a list of indices to each of its
  neighbours; this list will be empty for most of the neighbours, but
  we can not omit sending it.
\item Each processor then receives these lists from all others, and
  draws up lists of which indices to send.
\end{itemize}
You will note that, even though the communication during the
matrix-vector product involves only a few neighbours for each
processor, giving a cost that is $O(1)$ in the number of processors,
the setup involves all-to-all communications, which is $O(P)$ in the number of
processors.
The setup can be reduced to $O(\log P)$ with some
trickery~\cite{Falgout:scalable-hypre}.

\begin{exercise}
  The above algorithm for determining the communication part of the
  sparse matrix-vector product can be made far more efficient if we
  assume a matrix that is \indexterm{structurally symmetric}:
  $a_{ij}\not=0\Leftrightarrow a_{ji}\not=0$. Show that in this case
  no communication is needed to determine the communication pattern.
\end{exercise}

