\def\R{{\cal R}}

\Level 0 {Recommender systems}

The article 
\url{http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf}
describes systems such as from \indexterm{Netflix}
that try to predict how much a given person will like a given item.

The approach here is identify a, relatively low dimensional, space~$\R^f$
of \indexterm{features}. For movies one could for instance adopt two features:
low-brow versus high-brow, and historic vs contemporary.
(High-brow historic: Hamlet; low-brow historic: Robin Hood, men in tights; 
high-brow contemporary: anything by Godard; low-brow contemporary, eh, 
do I really need to find examples?)
For each item (movie)~$i$ there is then a vector $q_i\in\R^f$ that characterises
the item, and for each person there is a vector $p_j\in\R^f$ that
characterises how much that person likes such movies. The predicted rating
for a user of a movie is then $r_{ij}=q_i^tp_j$.

The vectors $q_i,p_j$ are gradually learned based on actual
ratings~$r_{ij}$, for instance by gradually minimizing
\[ \min_{p,q} \sum_{ij} (r_{ij}-q_i^tp_j)^2+
    \lambda \bigl( \|q_i\|^2+\|p_j\|^2 \bigr)
\]

\Level 1 {Stochastic gradient descent}

Let $e_{ij}=r_{ij}-q_i^tp_j$ be the difference between actual and predicted rating.
\[
\begin{array}{l}
  q_i\leftarrow q_i+\gamma (e_{ij}p_j-\lambda q_i)\\
  p_j\leftarrow p_j+\gamma (e_{ij}q_i-\lambda p_j)
\end{array}
\]

\Level 1 {Alternating least squares}

First fix all $p_j$ and minimize the $q_i$ (in parallel),
then fix $q_i$ and minimize the~$p_j$.
This handles missing data better.

%Machine learning, for an introduction to ML techniques; see~\cite{Tara:MLbiology}.
