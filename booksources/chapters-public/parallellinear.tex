% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Introduction to High-Performance Scientific Computing'
%%%% by Victor Eijkhout, copyright 2012
%%%%
%%%% This book is distributed under a Creative Commons Attribution 3.0
%%%% Unported (CC BY 3.0) license and made possible by funding from
%%%% The Saylor Foundation \url{http://www.saylor.org}.
%%%%
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we will discuss  a number of issues pertaining to
linear algebra on parallel computers. We will take a realistic view of
this topic, assuming that the number of processors is finite, and that
the problem data is always large, relative to the number of
processors. We will also pay attention to the physical aspects of the
communication network between the processors. 

We will analyze various linear algebra operations, including iterative
methods, and their behaviour in the presence of a network with finite
bandwidth and finite connectivity. This chapter will conclude with
various short remarks regarding complications in algorithms that arise
due to parallel execution.

\Level 0 {The sparse matrix-vector product}
\label{sec:spmvp-performance}

In linear system solving through iterative methods (see
section~\ref{sec:iterative}) the matrix-vector product is
computationally an important kernel, since it is executed in each of
potentially hundreds of iterations. In this section we look at
performance aspects of the matrix-vector product on a single
processor; the multi-processor case will get our attention in later
sections.

We are not much worried about the dense matrix-vector product in the
context of iterative methods, since one typically does not iterate on
dense matrices. In the case that we are dealing with block matrices,
refer to section~\ref{sec:mvp-opt} for an analysis of the dense
product. The sparse product is a lot trickier, since most of that
analysis does not apply.

\heading{Data reuse in the sparse matrix-vector product}

There are some similarities between the dense matrix-vector product,
executed by rows, and the \ac{CRS} sparse product
(section~\ref{sec:crs-mvp}). In both cases all matrix elements are
used sequentially, so any cache line loaded is utilized
fully. However, the \ac{CRS} product is worse at least the following
ways:
\begin{itemize}
\item The indirect addressing requires loading the elements of an
  integer vector. This implies that the sparse product has more memory
  traffic for the same number of operations.
\item The elements of the source vector are not loaded sequentially,
  in fact they can be loaded in effectively a random order. This means
  that a cacheline contains a source element will likely not be fully
  utilized. Also, the prefetch logic of the memory subsystem
  (section~\ref{sec:prefetch}) cannot assist here.
\end{itemize}
For these reasons, an application that is computationally dominated by
the sparse matrix-vector product can very well be run at $\approx5\%$
of the peak performance of the processor.

It may be possible to improve this performance if the structure of the
matrix is regular in some sense. One such case is where we are dealing
with a \indexterm{block matrix} consisting completely of small dense
blocks. This leads at least to a reduction in the amount of indexing
information: if the matrix consists of $2\times2$ blocks we get a
$4\times$ reduction in the amount of integer data transferred.
\begin{exercise}
  Give two more reasons why this strategy is likely to improve
  performance. Hint: cachelines, and reuse.
\end{exercise}
Such a \indextermbus{matrix}{tesselation} may give a factor of~2 in
performance improvement. Assuming such an improvement, we may adopt
this strategy even if the matrix is not a perfect block matrix: if
every $2\times2$ block will contain one zero element we may still get
a factor of~$1.5$ performance
improvement~\cite{vuduc:thesis,ButtEijkLang:spmvp}.

\heading{Vectorization in the sparse product}

In other circumstances bandwidth and reuse are not the dominant concerns:
\begin{itemize}
\item On old vector computers, such as old \indexterm{Cray} machines,
  memory was fast enough for the processor, but vectorization was
  paramount. This is a problem for sparse matrices, since the number
  of zeros in a matrix row, and therefore the vector length, is
  typically low.
\item On \acp{GPU} memory bandwidth is fairly high, but it is
  necessary to find large numbers of identical operations. Matrix can
  be treated independently, but since the rows are likely of unequal
  length this is not an appropriate source of parallelism.
\end{itemize}
For these reasons, a variation on the \indexterm{diagonal storage}
scheme for sparse matrices has seen a revival
recently. The observation here is that
if you sort the matrix rows by the number of rows you get a small
number of blocks of rows; each block will be fairly large, and in each
block the rows have the same number of elements.

A matrix with such a structure is good for vector
architectures~\cite{DAzevedo2005:vector-mvp}. In this case the product
is computed by diagonals.
\begin{exercise}
  Write pseudo-code for this case. How did the sorting of the rows
  improve the situation?
\end{exercise}

This sorted storage scheme also solves the problem we noted on
\acp{GPU}~\cite{Bolz:GPUsparse}. In this case we the traditional
\ac{CRS} product algorithm, and we have an amount of parallelism equal
to the number of rows in a block.

Of course there is the complication that we have permuted the matrix:
the input and output vectors will need to be permuted accordingly. If
the product operation is part of an iterative method, doing this
permutation back and forth in each iteration will probably negate any
performance gain. Instead we could permute the whole linear system and
iterate on the permuted system.
\begin{exercise}
  Can you think of reasons why this would work? Reasons why it wouldn't?
\end{exercise}

\Level 0 {Parallel dense matrix-vector product}
\label{sec:parallel-dense-mvp}
\index{Dense linear algebra|(}

In this section we will go into great detail into the performance, and
in particular the scalability, of the parallel dense matrix-vector
product. First we will consider a simple case, and discuss the
parallelism aspects in some amount of detail.

\Level 1 {Implementing the block-row case}
\label{sec:blockrow}

In designing a parallel version of an algorithm, one often proceeds by
making a \indexterm {data decomposition} of the objects involved. In
the case of a matrix-vector operations such as the product $y=Ax$, we
have the choice of starting with a vector decomposition, and exploring
its ramifications on how the matrix can be decomposed, or rather to
start with the matrix, and deriving the vector decomposition from it.
In this case, it seems natural to start with decomposing the matrix
rather than the vector, since it will be most likely of larger
computational significance. We now have two choices:
\begin{enumerate}
\item We make a one-dimensional decomposition of the matrix, splitting
  it in block rows or block columns, and assigning each of these --~or
  groups of them~-- to a processor.
\item Alternatively, we can make a two-dimensional decomposition,
  assigning to each processor one or more general submatrices.
\end{enumerate}

We start by considering the decomposition in block rows. Consider
a processor~$p$ and the set $I_p$ of indices of rows that it
owns\footnote{For ease of exposition we will let $I_p$ be a contiguous
  range of indices, but any general subset is allowed.}, and
let $i\in I_p$ be a row that is assigned to this processor. 
%(In
%illustrations we will let $I_p$ be a consecutive set, but this need
%not be true in general.) 
The elements in
row~$i$ are used in the operation
\[ y_i=\sum_ja_{ij}x_j \]
We now reason:
\begin{itemize}
\item If processor $p$ has all $x_j$ values, the matrix-vector product
  can trivially be executed, and upon completion, the processor has
  the correct values~$y_j$ for~$j\in I_p$.
\item This means that every processor needs to have a copy of~$x$,
  which is wasteful. Also it raises the question of data integrity:
  you need to make sure that each processor has the correct value
  of~$x$.
\item In certain practical applications (for instance iterative
  methods, as you have seen before), the output of the matrix-vector
  product is, directly or indirectly, the input for a next
  matrix-vector operation. This is certainly the case for the power
  method which computes $x, Ax, A^2x,\ldots$. Since our operation
  started with each processor having the whole of~$x$, but ended with
  it owning only the local part of~$Ax$, we have a mismatch.
\item Maybe it is better to assume that each processor, at the start
  of the operation, has only the local part of~$x$, that is,
  those~$x_i$ where~$i\in I_p$, so that the start state and end state
  of the algorithm are the same. This means we have to change the
  algorithm to include some communication that allows each processor
  to obtain those values~$x_i$ where~$i\not\in\nobreak I_p$.
\end{itemize}

\begin{exercise}
  Go through a similar reasoning for the case where the matrix is
  decomposed in block columns. Describe the parallel algorithm in
  detail, like above, but without giving pseudo code.
\end{exercise}

\def\sublocal{_{\mathrm\scriptstyle local}}

Let us now look at the communication in detail: we will consider a
fixed processor~$p$ and consider the operations it performs and the
communication that necessitates.
According to the above analysis,
in executing the statement $y_i=\sum_ja_{ij}x_j$ we have
to be aware what processor the $j$ values `belong to'. To acknowledge
this, we write
\begin{equation}
  y_i=\sum_{j\in I_p}a_{ij}x_j+\sum_{j\not\in I_p}a_{ij}x_j
  \label{eq:yi=sum-in-and-not}
\end{equation}
If $j\in I_p$, the instruction $y_i \leftarrow y_i + a_{aij} x_j$
involves only quantities that are already local to
the processor.
Let us therefore concentrate on the case
$j\not\in I_p$.
It would be nice if we could just write the statement
\begin{verbatim}
y(i) = y(i) + a(i,j)*x(j)
\end{verbatim}
and some lower layer would automatically transfer \verb+x(j)+ from
whatever processor it is stored on to a local register. (The PGAS
languages of section~\ref{sec:pgas} aim to do this, but their
efficiency is far from guaranteed.) An implementation, based on this
optimistic view of parallelism, is given in figure~\ref{fig:naive-pmvp}.

\begin{figure}
  \begin{displayprocedure}{Naive Parallel MVP}{$A,x\sublocal,y\sublocal,p$}
    \KwIn{Processor number~$p$; the elements $x_i$ with $i\in I_p$; matrix
      elements $A_{ij}$ with $i\in I_p$.}
    \KwOut{The elements $y_i$ with $i\in I_p$}
    \For{$i\in I_p$}{$s\leftarrow0$\;
      \For{$j\in I_p$}{$s\leftarrow s+a_{ij}x_{j}$}
      \For{$j\not\in I_p$}{send $x_j$ from the processor that owns it to
        the current one, then\;  $s\leftarrow s+a_{ij}x_{j}$}
      $y_i\leftarrow s$
    }
  \end{displayprocedure}
  \caption{A na\"\i vely coded parallel matrix-vector product}
  \label{fig:naive-pmvp}
\end{figure}

The immediate problem with such a
`local' approach is that too much communication will take place.
\begin{itemize}
\item If the matrix $A$ is dense, the element $x_j$ is necessary once
  for each row $i\in I_p$, and it will thus be fetched once for every
  row~$i\in I_p$.
\item For each processor $q\not=p$, there will be (large) number of
  elements $x_j$ with $j\in I_q$ that need to be transferred from
  processor~$q$ to~$p$. Doing this in separate messages, rather than
  one bulk transfer, is very wasteful.
\end{itemize}
With shared memory these issues are not much of a problem, but in the
context of distributed memory it is better to take a
\indexterm{buffering} approach.

Instead of communicating individual elements of~$x$, we use a local
buffer $B_{pq}$ for each processor~$q\not=p$ where we collect the
elements from~$q$ that are needed to perform the product on~$p$. (See
\begin{figure}
  \begin{quote}
    \includegraphics[scale=.12]{graphics-public/distmvp}
  \end{quote}
  \caption{The parallel matrix-vector product with a blockrow
    distribution.}
  \label{fig:distmvp2}
\end{figure}
figure~\ref{fig:distmvp2} for an illustration.) The parallel algorithm
is given in figure~\ref{fig:buffer-pmvp}.
\begin{figure}
\begin{displayprocedure}{Parallel MVP}{$A,x\sublocal,y\sublocal,p$}
  \KwIn{Processor number~$p$; the elements $x_i$ with $i\in I_p$; matrix
    elements $A_{ij}$ with $i\in I_p$.}
  \KwOut{The elements $y_i$ with $i\in I_p$}
  \For{$q\not=p$}{Send elements of~$x$ from processor $q$ to~$p$,
    receive in buffer~$B_{pq}$.}
  $y\sublocal\leftarrow A x\sublocal$\\
  \For{$q\not=p$}{$y\sublocal\leftarrow y\sublocal+A_{pq}B_q$}
\end{displayprocedure}
  \caption{A buffered implementation of the parallel matrix-vector
    product}
  \label{fig:buffer-pmvp}
\end{figure}

In addition to preventing an element from being fetched more than
once, this also combines many small messages into one large message,
which is usually more efficient; recall our discussion of bandwidth
and latency in section~\ref{sec:bwlatency}.

\begin{exercise}
  Give pseudocode for the matrix-vector product using 
  nonblocking operations (section~\ref{sec:nonblocking})
\end{exercise}

Above we said that having a copy of the whole of~$x$ on each processor
was wasteful in space. The implicit argument here is that, in general,
we do not want local storage to be function of the number of
processors: ideally it should be only a function of the local
data. (This is related to weak scaling; section~\ref{sec:scaling}.)

You see that, because of communication considerations, we have
actually decided that it is unavoidable, or at least preferable, for
each processor to store the whole input vector.  Such trade-offs
between space and time efficiency are fairly common in parallel
programming. For the dense matrix-vector product we can actually
defend this overhead, since the vector storage is of lower order than
the matrix storage, so our over-allocation is small by ratio. Below
(section~\ref{sec:pspmvp}), we will see that for the sparse
matrix-vector product the overhead can be much less.

It is easy to see that the parallel dense matrix-vector product, as
described above, has perfect speedup \emph{if we are allowed to ignore
the time for communication}. In the next couple of sections you will
see that the block row implementation above is not optimal if we take
communication into account. For scalability we need a two-dimensional
decomposition. We start with a discussion of collectives.

\Level 1 {Scalability of the dense matrix-vector product}
\label{sec:densescaling}

\input chapters-public/mvpscaling2

\Level 0 {Scalability of LU factorization}
\label{sec:LUscaling}

A full analysis of the scalability of dense LU factorization is quite
involved, so we will state without further proof that again a
two-dimensional distribution is needed. However, we can identify a
further complication. Since factorizations of any
type\footnote{Gaussian elimination can be performed in right-looking,
  left-looking and Crout variants; see~\cite{TSoPMC}.}
progress through a matrix, processors will be inactive for part of the
time.

\begin{exercise}
  Consider the regular right-looking Gaussian elimination
\begin{verbatim}
for k=1..n
  p = 1/a(k,k)
  for i=k+1,n
    for j=k+1,n
      a(i,j) = a(i,j)-a(i,k)*p*a(k,j)
\end{verbatim}
  Analyze the running time, speedup, and efficiency as a function
  of~$N$, if we assume a one-dimensional distribution, and enough
  processors to store one column per processor. Show that speedup is
  limited.

  Also perform this analysis for a two-dimensional decomposition where
  each processor stores one element.
\end{exercise}

\index{matrix!storage, dense|(}

For this reason, an \indexterm{overdecomposition} is used, where
the matrix is divided in more blocks than there are processors,
and each processor stores several, non-contiguous, sub-matrices. 
\begin{figure}[ht]
  \includegraphics[scale=.11]{graphics-public/cyclic-1}
  \caption{One-dimensional cyclic distribution: assignment of four
    matrix columns to two processors, and the resulting mapping of
    storage to matrixcolumns}
  \label{fig:cyclic-1}
\end{figure}
We illustrate this in figure~\ref{fig:cyclic-1} where we divide four
block columns of a matrix to two processors: each processor stores in
a contiguous block of memory two non-contiguous matrix columns.

Next, we illustrate in figure~\ref{fig:cyclic-1-mvp}
\begin{figure}[ht]
  \includegraphics[scale=.11]{graphics-public/cyclic-1-mvp}
  \caption{Matrix-vector multiplication with a cyclicly distributed matrix}
  \label{fig:cyclic-1-mvp}
\end{figure}
that a matrix-vector product with such a matrix can be performed
without knowing that the processors store non-contiguous parts of the
matrix. All that is needed is that the input vector is also cyclicly
distributed.

\begin{exercise}
  Now consider a $4\times4$ matrix and a $2\times2$ processor
  grid. Distribute the matrix cyclicly both by rows and columns. Show
  how the matrix-vector product can again be performed using the
  contiguous matrix storage, as long as the input is distributed
  correctly. How is the output distributed? Show that more
  communication is needed than the reduction of the one-dimensional
  example.
\end{exercise}

Specifically,
with $P<N$ processors, and assuming for simplicity $N=cP$, we let
processor~0 store rows $0,c,2c,3c,\ldots$; processor~1 stores rows
$1,c+1,2c+1,\ldots$, et cetera. This scheme can be generalized two a
two-dimensional distribution, if $N=c_1P_1=c_2P_2$
and~$P=P_1P_2$. This is called a 2D \indexterm{cyclic
  distribution}. This scheme can be further extended by considering
block rows and colums (with a small block size), and assigning to
processor~0 the \emph{block} rows $0,c,2c,\ldots$.

\begin{exercise}
  Consider a square $n\times n$ matrix, and a square $p\times p$
  processor grid, where $p$ divides~$n$ without remainder. Consider
  the overdecomposition outlined above, and make a sketch of matrix
  element assignment for the specific case $n=6,p=2$. That is, draw an
  $n\times n$ table where location $(i,j)$ contains the processor
  number that stores the corresponding matrix element. Also make a
  table for each of the processors describing the local to global mapping,
  that is, giving the global $(i,j)$ coordinates of the elements in
  the local matrix. (You will find this task facilitated by using
  zero-based numbering.)

  Now write functions $P,Q,I,J$ of $i,j$ that describe the global to
  local mapping, that is, matrix element $a_{ij}$ is stored in
  location $(I(i,j),J(i,j))$ on processor $(P(i,j),Q(i,j))$.
\end{exercise}

\index{matrix!storage, dense|)}

\index{Dense linear algebra|)}

\Level 0 {Parallel sparse matrix-vector product}
\label{sec:pspmvp}
\index{matrix-vector product!sparse|see{sparse, matrix-vector product}}
\index{sparse!matrix-vector product!parallel|(}
\input chapters-public/spmvp
\index{sparse!matrix-vector product!parallel|)}

\begin{notready}
\Level 0 {Parallelism in solving linear systems from \acp{PDE}}

The numerical solution of \acp{PDE} is an important activity,
and the precision required often makes it a prime candidate for
parallel treatment. If we wonder just how parallel we can be,
and in particular what sort of a speedup is attainable,
we need to distinguish between various aspects of the question.

First of all we can ask if there is any intrinsic parallelism
in the problem. On a global level this will typically not be the case
(if parts of the problem were completely uncoupled, then they would be
separate problems, right?) but on a smaller level there may be
parallelism. 

For instance, looking at time-dependent problems, and referring to
section~\ref{sec:region-influence}, we can say that every next time
step is of course dependent on the previous one, but not every
individual point on the next time step is dependent on every point in
the previous step: there is a \indexterm{region of influence}. 
Thus it may be possible to partition the problem domain and
obtain parallelism.
\end{notready}


\Level 0 {Computational aspects of iterative methods}
\label{sec:iterative-computational}

All iterative methods feature the following operations:
\begin{itemize}
\item A matrix-vector product; this was discussed for the sequential
  case in section~\ref{sec:sparse} and for the parallel case in
  section~\ref{sec:pspmvp}.
  In the parallel case,
  construction of \ac{FEM} matrices has a complication that we will
  discuss in section~\ref{sec:fem-assembly}.
\item The construction of the preconditioner matrix~$K\approx A$, and
  the solution of systems $Kx=y$. This was discussed in the sequential
  case in section~\ref{sec:preconditioner}. Below
  we will go into parallelism aspects in section~\ref{sec:parallel-prec}.
\item Some vector operations (including inner products, in
  general). These will be discussed next.
\end{itemize}

\Level 1 {Vector operations}

There are two types of vector operations in a typical iterative method:
vector additions and inner products. 

\begin{exercise}
  Consider the \ac{CG} method of section~\ref{sec:cg},
  figure~\ref{fig:pcg}, applied to the
  matrix from a 2D \ac{BVP}; equation~\eqref{eq:5starmatrix}, First
  consider the unpreconditioned case $M=I$. Show that there is a
  roughly equal number of floating point
  operations are performed in the matrix-vector product and 
  in the vector operations. Express everything in the matrix size~$N$ and
  ignore lower order terms. How would this balance be if the matrix
  had 20 nonzeros per row?

  Next, investigate this balance between vector and matrix operations
  for the \ac{FOM} scheme in section~\ref{sec:fom}. Since the number
  of vector operations depends on the iteration, consider the first 50
  iterations and count how many floating point operations are done in
  the vector updates and inner product versus the matrix-vector
  product. How many nonzeros does the matrix need to have for these
  quantities to be equal?
\end{exercise}

\begin{exercise}
  Flop counting is not the whole truth. What can you say about the
  efficiency of the vector and matrix operations in an iterative
  method, executed on a single processor?
\end{exercise}

\Level 2 {Vector additions}

The vector additions are
typically of the form $x\leftarrow x+\alpha y$ or $x\leftarrow \alpha x+y$.
If we assume that all vectors are distributed the same way, this
operation is fully parallel.

\Level 2 {Inner products}
\index{inner products|(}

Inner products are vector operations, but they are computationally
more interesting than updates, since they involve communication. 

When we compute an inner product, most likely 
every processor needs to receive the computed value. We
use the following algorithm:

\begin{displayalgorithm}
  \For {processor $p$} {
    compute $a_p\leftarrow x_p^ty_p$ where $x_p,y_p$ are the part of
    $x,y$ stored on processor~$p$ }
    do a global reduction to compute $a=\sum_p a_p$ \;
    broadcast the result
  \caption{Compute $a\leftarrow x^ty$ where $x,y$ are distributed vectors}
\end{displayalgorithm}

The reduction and broadcast (which can be joined into an {\tt
  Allreduce}) combine data over all processors, so they have a
communication time that increases with the number of processors. This
makes the inner product potentially an expensive operation, and people
have suggested a number of ways to reducing their impact on the
performance of iterative methods.

\begin{exercise}
  Iterative methods are typically used for sparse matrices. In that
  context, you can argue that the communication involved in an inner product
  can have a larger influence on overall performance than the
  communication in the matrix-vector product. What is the
  complexity of the matrix-vector product and the inner product as a
  function of the number of processors?
\end{exercise}

Here are some of the approaches that have been taken.
\begin{itemize}
\item The \ac{CG} method has two inner products per iteration that are
  inter-dependent. It is possible to rewrite the method so that it
  computes the same iterates (in exact arithmetic, at least) but so
  that the two inner products per iteration can be
  combined. See~\cite{ChGe:sstep,DAzEijRo:ppscicomp,Me:multicg,YandBrent:bicgstab}.
\item It may be possible to overlap the inner product calculation with
  other, parallel, calculations~\cite{dehevo92:acta}.
\item In the \ac{GMRES} method, use of the classical \acf{GS}
  method takes far fewer independent inner product than the modified
  \ac{GS} method, but it is less stable. People have investigated strategies for deciding
  when it is allowed to use the classic \ac{GS} method~\cite{Langou:thesis}.
\end{itemize}

\index{inner products|)}

\Level 1 {Finite element matrix construction}
\label{sec:fem-assembly}

The \indexac{FEM} leads to an interesting issue in parallel
computing. For this we need to sketch the basic outline of how this
method works. The \ac{FEM} derives its name from the fact that the
physical objects modeled are divided into small two or three
dimensional shapes, the elements, such as triangles and squares in 2D,
or pyramids and bricks in~3D. On each of these, the function we are
modeling is then assumed to polynomial, often of a low degree, such as
linear or bilinear.

\begin{figure}[ht]
  \includegraphics[scale=.12]{graphics-public/fem}  
  \caption{A finite element domain, parallelization of the matrix
    construction, and parallelization of matrix element storage}
  \label{fig:fem-assembly}
\end{figure}

The crucial fact is that a matrix element~$a_{ij}$ is then the sum of
computations, specifically certain integrals, over all elements that
contain both variables $i$ and~$j$:
\[ a_{ij}=\sum_{e\colon i,j\in e} a^{(e)}_{ij}. \]
The computations in each element share many common parts, so it is
natural to assign each element~$e$ uniquely to a processor~$P_e$,
which then computes all contributions~$a^{(e)}_{ij}$. In
figure~\ref{fig:fem-assembly} element~2 is assigned to processor~0
and element~4 to processor~1.

Now consider variables $i$ and~$j$ and the matrix
element~$a_{ij}$.  It is constructed as the sum of computations over
domain elements 2 and~4, which have been assigned to different processors.
Therefore, no matter what processor row $i$ is assigned to, at least
one processor will have to communicate its contribution to matrix
element~$a_{ij}$.

Clearly it is not possibly to make assignments $P_e$ of elements and
$P_i$ of variables such that $P_e$ computes in full the coefficients
$a_{ij}$ for all $i\in e$. In other words, if we compute the
contributions locally, there needs to be some amount of communication to
assemble certain matrix elements.
For this reason, modern linear algebra libraries such as PETSc (see
tutorial section~\ref{tut:petsc}) allow any processor to set any
matrix element.

\Level 0 {Parallel preconditioners}
\label{sec:parallel-prec}

Above (sections \ref{sec:preconditioner} and \ref{sec:ilu}) we saw a
couple of different choices of~$K$. In this section we will begin the
discussion of parallelization strategies. The discussion is continued
in detail in the next sections.

\Level 1 {Jacobi preconditioning}

The Jacobi method (section~\ref{sec:jacobi-seidel}) uses the diagonal
of~$A$ as preconditioner. Applying this is as parallel as is
possible: the statement $y\leftarrow K\inv x$ scales every element of
the input vector independently. Unfortunately the improvement in the
number of iterations with a Jacobi preconditioner is rather
limited. Therefore we need to consider more sophisticated methods such
\ac{ILU}. Unlike with the Jacobi preconditioner, parallelism is then
not trivial.

\Level 1 {The trouble with parallel ILU}

Above we saw that, in a flop counting sense, applying an ILU
preconditioner (section~\ref{sec:ilu}) is about as expensive as doing
a matrix-vector product. This is no longer true if we run our
iterative methods on a parallel computer.

At first glance the operations are similar. A matrix-vector product
$y=Ax$ looks like
\begin{verbatim}
for i=1..n
  y[i] = sum over j=1..n a[i,j]*x[j]
\end{verbatim}
In parallel this would look like
\begin{verbatim}
for i=myfirstrow..mylastrow
  y[i] = sum over j=1..n a[i,j]*x[j]
\end{verbatim}
Suppose that a processor has local copies of all the elements of $A$
and~$x$ that it will need, then this operation is fully parallel: each
processor can immediately start working, and if the work load is
roughly equal, they will all finish at the same time. The total time
for the matrix-vector product is then divided by the number of
processors, making the speedup more or less perfect.

Consider now the forward solve $Lx=y$, for instance in the context of
an \ac{ILU} preconditioner:
\begin{verbatim}
for i=1..n
  x[i] = (y[i] - sum over j=1..i-1 ell[i,j]*x[j]) / a[i,i]
\end{verbatim}
We can simply write the parallel code:
\begin{verbatim}
for i=myfirstrow..mylastrow
  x[i] = (y[i] - sum over j=1..i-1 ell[i,j]*x[j]) / a[i,i]
\end{verbatim}
but now there is a problem. We can no longer say `suppose a processor
has local copies of everything in the right hand side', since the
vector~$x$ appears both in the left and right hand side. While the
matrix-vector product is in principle fully parallel over the matrix
rows, this triangular solve code is recursive, hence sequential.

In a parallel computing context this means that, for the second
processor to start, it needs to wait for certain components of~$x$
that the first processor computes. Apparently, the second processor
can not start until the first one is finished, the third processor has
to wait for the second, and so on. The disappointing conclusion is
that in parallel only one processor will be active at any time, and
the total time is the same as for the sequential algorithm. This is
actually not a big problem in the dense matrix case, since parallelism
can be found in the operations for handling a single row (see
section~\ref{sec:multicore-block}), but in the sparse case it means we
can not use incomplete factorizations without some redesign.

In the next few subsections we will see different strategies for
finding preconditioners that perform efficiently in parallel.

\Level 1 {Block Jacobi methods}
\label{sec:block-jacobi}
\index{block Jacobi|(}

Various approaches have been suggested to remedy this sequentiality
the triangular solve. For instance, we could simply let the processors
ignore the components of~$x$ that should come from other processors:
\begin{verbatim}
for i=myfirstrow..mylastrow
  x[i] = (y[i] - sum over j=myfirstrow..i-1 ell[i,j]*x[j]) 
         / a[i,i]
\end{verbatim}
This is not mathematically equivalent to the sequential algorithm
(technically, it is called a \indexterm{block Jacobi} method with 
\ac{ILU} as the \indexterm{local solve}), but
since we're only looking for an approximationg $K\approx A$, this is
simply a slightly cruder approximation.
\begin{exercise}
  Take the Gauss-Seidel code you wrote above, and simulate a parallel
  run. What is the effect of increasing the (simulated) number of processors?
\end{exercise}

The idea behind block methods can easily be appreciated pictorially;
see figure~\ref{fig:block-method}.
\begin{figure}[ht]
  \includegraphics[scale=.12]{graphics-public/block-jacobi}
  \caption{Sparsity pattern corresponding to a block Jacobi
    preconditioner}
  \label{fig:block-method}
\end{figure}
In effect, we make an \ac{ILU} of the matrix that we get by ignoring
all connections between processors. Since in a \ac{BVP} all points
influence each other (see section~\ref{sec:region-influence}), using a
less connected preconditioner will increase the number of iterations
if executed on a sequential computer. However, block methods are
parallel and, as we observed above, a sequential preconditioner is
very inefficient in a parallel context, so we put up with this
increase in iterations.

\index{block Jacobi|)}

\Level 1 {Parallel ILU}
\label{sec:parallel-ilu}
\index{Incomplete LU (ILU)!parallel|(}

The Block Jacobi preconditioner operates by decoupling domain
parts. While this may give a method that is highly parallel, it may
give a higher number of iterations than a true \ac{ILU}
preconditioner. (A~theoretical argument can be made that this
decoupling decreases the efficiency of the iterative method; see
section~\ref{sec:region-influence}.) Fortunately it is possible to
have a parallel \ac{ILU} method.

In section~\ref{sec:redblackgreen} you saw the combination of
\indextermbus{graph}{colouring} and permutation. Let $P$ be the
permutation that groups like-coloured variables together, then $\tilde
A=P^tAP$ is a matrix with the following structure:
\begin{itemize}
\item $\tilde A$ has a block structure with the number of blocks equal
  to the number of colours in the adjacency graph of~$A$; and
\item each diagonal block is a diagonal matrix.
\end{itemize}
Now, if you are performing an iterative system solution
and you are looking for a parallel preconitioner you can use this
permuted matrix. Consider solving $Ly=x$ with the permuted system. We
write the usual algorithm (section~\ref{sec:lu-solve}) as
\begin{quotation}
  \begin{tabbing}
    for \=$c$ in the set of colours:\\
    \>for \=$i$ in the variables of colour $c$:\\
    \>\>$y_i\leftarrow x_i-\sum_{j<i} \ell_{ij}y_j$
  \end{tabbing}
\end{quotation}

\begin{exercise}
  Show that the flop count of solving a system $LUx=y$ remains the
  same (in the highest order term) when you from an \ac{ILU}
  factorization in the natural ordering to one in the colour-permuted
  ordering.
\end{exercise}

\begin{figure}
  \includegraphics[scale=.1]{graphics-public/pilu}
  \caption{A partitioned domain with coloured nodes}
  \label{fig:pilu}
\end{figure}
Where does all this colouring get us? Solving is still
sequential\ldots Well, it is true that the outer loop over the colours
is sequential, but all the points of one colour are independent of
each other, so they can be solved at the same time.
\begin{figure}
  \includegraphics[scale=.12]{graphics-public/pilu-solve}
  \caption{Solving a parallel multicolour ILU in four steps}
  \label{fig:pilu-solve}
\end{figure}
So if we use an ordinary domain partitioning and combine that with a
multi-colouring (see figure~\ref{fig:pilu}), the processors are all
active during all the colour stages; see
figure~\ref{fig:pilu-solve}. Ok, if you take a close look at that
figure you'll see that one processor is not active in the last
colour. With large numbers of nodes per processor this is unlikely to
happen, but there may be some load inbalance.

\index{Incomplete LU (ILU)!parallel|)}

\Level 0 {Ordering strategies and parallelism}
\label{sec:ordering}

In the foregoing we have remarked on the fact that solving
a linear system of equations is inherently a recursive activity.
For dense systems, the number of operations is large enough
compared to the recursion length that finding parallelism 
is fairly straightforward. Sparse systems, on the other hand,
take more sophistication. In this section we will look at 
a number of strategies for reordering the equations (or, equivalently,
permuting the matrix) that will increase the available parallelism.

These strategies can all be considered as variants of Gaussian elimination.
By making incomplete variants of them (see section~\ref{sec:ilu}),
all these strategies also apply to constructing preconditioners
for iterative solution methods.

\Level 1 {Nested dissection}
\label{sec:dissection}
\index{nested dissection|(}
\input chapters-public/dissection
\index{nested dissection|)}
\Level 1 {Variable reordering and colouring: independent sets}
\label{sec:redblackgreen}
\input chapters-public/redblackgreen

\Level 0 {Operator splitting}

In some contexts, it is necessary to perform implicit calculations
through all directions of a two or three-dimensional array. For
example, in section~\ref{sec:heateq} you saw how the implicit solution
of the heat equation
gave rise to repeated systems
\begin{equation}
  (\alpha I+\frac {d^2}{dx^2}+\frac{d^2}{dy^2})u^{(t+1)}=u^{(t)}
  \label{eq:heat-recap}
\end{equation}
Without proof, we state that the time-dependent problem can also be solved by
\begin{equation}
  (\beta I+\frac {d^2}{dx^2})(\beta I+\frac{d^2}{dy^2})u^{(t+1)}=u^{(t)}
  \label{eq:adi-recap}
\end{equation}
for suitable~$\beta$. This scheme will not compute the same
values on each individual time step, but it will converge to the same
steady state. The scheme can also be used as a preconditioner in the
\ac{BVP} case.

This approach has considerable advantages, mostly in terms of
operation counts: the original system has to be solved either making a
factorization of the matrix, which incurs \indexterm{fill-in}, or by
solving it iteratively.

\begin{exercise}
  Analyze the relative merits of these approaches, giving rough
  operation counts. Consider both the case where $\alpha$ has
  dependence on~$t$ and where it does not. Also discuss the expected
  speed of various operations.
\end{exercise}

A further advantage appears when we consider the parallel solution
of~\eqref{eq:adi-recap}. Note that we have a two-dimensional set of
variables~$u_{ij}$, but the operator $I+d^2u/dx^2$ only connects
$u_{ij},u_{ij-1},u_{ij+1}$. That is, each line corresponding to an
$i$~value can be processed independently. Thus, both operators can be
solved fully parallel using a one-dimensional partition on the domain.
The solution of a the system in~\eqref{eq:heat-recap}, on the other
hand, has limited parallelism.

Unfortunately, there is a serious complication: the operator in $x$
direction needs a partitioning of the domain in on direction, and the
operator in $y$ in the other. The solution usually taken is to
transpose the $u_{ij}$ value matrix in between the two solves, so that
the same processor decomposition can handle both. This transposition
can take a substantial amount of the processing time of each time step.

\begin{exercise}
  Discuss the merits of and problems with a two-dimensional
  decomposition of the domain, using a grid of $P=p\times p$
  processors. Can you suggest a way to ameliorate the problems?
\end{exercise}

One way to speed up these calculations, is to replace the implicit
solve, by an explicit operation; see
section~\ref{sec:implicit-becomes-explicit}.

\Level 0 {Parallelism and implicit operations}
\input chapters-public/implicit

\Level 0 {Simulation scaling}

In section~\ref{sec:scaling} we considered weak scaling,
and we assumed that the amount of work and 
the amount of storage are linearly related. This is not always the case; for instance
the operation complexity of a matrix-matrix product is $N^3$ for $N^2$ data.
If you linearly increase the number of processors, the work will go up with a higher power.

A similar effect comes into play if you similate time-dependent \acp{PDE}.
Here, the total work is a product of the work per time step and the number of 
time steps. These two numbers are related; in section~\ref{sec:fd-ode} you
saw that the time step has a certain minimum size as a function of the 
space discretization. Thus, the number of time steps will go up as the work per
time step goes up.

Rather than investigating scalability from the point of the running
of an algorithm, in this section we will look at the case where the simulated time~$S$
and the running time~$T$ are constant, and we look at how this influences the amount of
memory we need. This corresponds to the following real-life scenario: 
you have a simulation that models a certain amount of real-world time 
in a certain amount of running time; now you buy a bigger computer, and you wonder
what size problem you can solve in the same running time and maintaining
the same simulated time.

Let $m$ be the memory per processor, and $P$ the number of processors, giving:
\[ M=Pm\qquad\hbox{total memory.} \]
If $d$ is the number of space dimensions of the problem, typically 2~or~3,
we get
\[ \Delta x = 1/M^{1/d}\qquad\hbox{grid spacing.} \]
For stability this limits the time step $\Delta t$ to
\[ \Delta t=
\begin{cases}
\Delta x=1\bigm/M^{1/d}&\hbox{hyperbolic case}\\
\Delta x^2=1\bigm/M^{2/d}&\hbox{parabolic case}
\end{cases}
\]
(noting that the hyperbolic case was not discussed in chapter~\ref{ch:odepde}.)
With a simulated time $S$, we find
\[ k=S/\Delta t\qquad \hbox{time steps.} \]
If we assume that the individual time steps are perfectly parallelizable,
that is, we use explicit methods, or implicit methods with optimal solvers,
we find a running time
\[ T=kM/P=\frac{S}{\Delta t}m. \]
Setting $T/S=C$, we find
\[ m=C\Delta t, \]
that is, the amount of memory per processor goes down as we increase the processor
count. (What is the missing step in that last sentence?)

Further analyzing this result, we find
\[ m=C\Delta t = c
\begin{cases}
1\bigm/M^{1/d}&\hbox{hyperbolic case}\\
1\bigm/M^{2/d}&\hbox{parabolic case}
\end{cases}
\]
Substituting $M=Pm$, we find ultimately
\[ m = C
\begin{cases}
1\bigm/P^{1/(d+1)}&\hbox{hyperbolic}\\
1\bigm/P^{2/(d+2)}&\hbox{parabolic}
\end{cases}
\]
that is, the memory per processor that we can use
goes down as a higher power of the number of processors.

\Level 0 {Grid updates}

One of the conclusions of chapter~\ref{ch:odepde} was that explicit methods
for time-dependent problems are computationally easier than implicit ones.
For instance,
they typically involve a matrix-vector product rather than a system solution,
and
parallelizing explicit operations is fairly simple: each result value of the 
matrix-vector product can be computed independently. That does not mean that
there are other computational aspects worth remarking on.

Since we are dealing with sparse matrices, stemming from some computational stencil,
we take the operator point of view. In figures \ref{fig:laplaceparallel} 
and~\ref{fig:ghost} you saw how applying a stencil in each point of the domain 
induces certain relations between processors: in order to evaluate the matrix-vector
product $y\leftarrow Ax$ on a processor, that processor needs to obtain the $x$-values
of its \indexterm{ghost region}. Under reasonable assumptions on the partitioning
of the domain over the processors, the number of messages involved will be fairly
small.

\begin{exercise}
Reason that, in a \ac{FEM} or \ac{FDM} context,
the number of messages is $O(1)$ as~$h\downarrow\nobreak 0$.
\end{exercise}

In section~\ref{sec:reuse} you saw that the matrix-vector product has
little data reuse, though there is some locality to the computation;
in section~\ref{sec:crs-mvp} it was pointed out that the locality of the 
sparse matrix-vector product is even worse because of
indexing schemes that the sparsity necessitates. This means that the sparse
product is largely a \emph{bandwidth-bound algorithm}. 

Looking at just a
single product there is not much we can do about that. 
However, 
often we do a number of such products in a row, for instance as the steps
in a time-dependent process. In that case there may be rearrangements
of the operations that lessen the bandwidth demands. Consider as a simple example
\begin{equation}
\forall_i\colon x^{(n+1)}_i = f\bigl( x^{(n)}_i, x^{(n)}_{i-1}, x^{(n)}_{i+1} \bigr)
\label{eq:3p-average}
\end{equation}

and let's assume that the set $\{x^{(n)}_i\}_i$ is too large to fit 
in cache.
This is a model for, for instance, the explicit scheme for the heat
equation in one space dimension; section~\ref{fig:explicit-heat}.
Schematically:
\[
\begin{array}{ccccc}
  x^{(n)}_0&x^{(n)}_1&x^{(n)}_2\\
  \downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow\\
  x^{(n+1)}_0&x^{(n+1)}_1&x^{(n+1)}_2\\
  \downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow&\searrow\,\downarrow\,\swarrow\\
  x^{(n+2)}_0&x^{(n+2)}_1&x^{(n+2)}_2\\
\end{array}
\]
In the ordinary computation, where we first compute all~$x^{(n+1)}_i$, 
then all~$x^{(n+2)}_i$, the intermediate values at level~$n+1$
will be flushed from the cache
after they were generated, and then brought back into cache as input for the
level $n+2$ quantities.

However,
if we compute not one, but two iterations, the intermediate values
may stay in cache.
Consider $x^{(n+2)}_0$: it requires $x^{(n+1)}_0,x^{(n+1)}_1$,
which in turn require $x^{(n)}_0,\ldots,x^{(n)}_2$.

Now suppose that we are not interested in the intermediate results, but
only the final iteration. Figure~\ref{fig:grid-update-overlap} shows
a simple example.
\begin{figure}[ht]
\includegraphics[scale=.1]{graphics-public/grid-update-overlap}
\caption{Computation of blocks of grid points over multiple iterations}
\label{fig:grid-update-overlap}
\end{figure}
The first processor computes 4~points on level $n+2$. For this it needs 5~points
from level $n+1$, and these need to be computed too, from 6~points on level~$n$.
We see that a processor apparently needs to collect a \indexterm{ghost region}
of width two, as opposed to just one for the regular single step update.
One of the points computed by the first processor is $x^{(n+2)}_3$,
which needs $x^{(n+1)}_4$. This point is also needed for the computation
of $x^{(n+2)}_4$, which belongs to the second processor.

The easiest solution is to let this sort of point on the intermediate
level \emph{redundantly computed}\index{redundant computation}, in 
both the computation of both blocks where it is needed.

\begin{exercise}
  Can you think of cases where a point would be redundantly computed by
  more than two processors?
\end{exercise}

We can give several interpretations to this scheme of computing multiple
update steps by blocks. First of all, as we motivated above, doing this 
on a single processor increases locality: if all points in a coloured block
(see the figure) fit in cache, we get reuse of the intermediate points.

Secondly, if we consider this as a scheme for distributed memory computation,
it reduces message traffic. Normally, for every update step the processors
need to exchange their boundary data. If we accept some redundant duplication
of work, we can now eliminate the data exchange for the intermediate levels.
The decrease in communication will typically outweigh the increase in work.

\begin{exercise}
  Discuss the case of using this strategy for multicore computation.
  What are the savings? What are the potential pitfalls?
\end{exercise}

Let's analyze the algorithm we have just sketched.  As in
equation~\eqref{eq:3p-average} we limit ourselves to a 1D set of
points and a function of three points. The parameters describing the
problem are these:
\begin{itemize}
\item $N$ is the number of points to be updated, and $M$~denotes the
  number of update steps. Thus, we perform $MN$ function evaluations.
\item $\alpha,\beta,\gamma$ are the usual parameters describing
  latency, transmission time of a single point, and time for an
  operation (here taken to be an $f$ evaluation).
\item $b$ is the number of steps we block together.
\end{itemize}
Each halo communication consists of $b$ points, and we do this $\sqrt
N/b$ many times.  The work performed consists of the $MN/p$ local
updates, plus the redundant work because of the halo. The latter term
consists of $b^2/2$ operations, performed both on the left and right
side of the processor domain.

Adding all these terms together, we find a cost of
\[ \frac Mb\alpha+M\beta+\left(\frac {MN}p+Mb\right)\gamma. \]
We observe that the overhead of $\alpha M/b+\gamma Mb$ is independent of~$p$,
\begin{exercise}
  Compute the optimal value of~$b$, and remark that it only depends on
  the architectural parameters $\alpha,\beta,\gamma$ but not on the
  problem parameters.
\end{exercise}

We can make this algorithm more efficient by overlapping the
communication and computation. As illustrated in
figure~\ref{fig:grid-update-local}, each processor start by
communicating its halo, and overlapping this communication with the
part of the communication that can be done locally. The values that
depend on the halo will then be computed last.

\begin{figure}[ht]
\includegraphics[scale=.1]{graphics-public/grid-update-local}
\caption{Computation of blocks of grid points over multiple iterations}
\label{fig:grid-update-local}
\end{figure}

\begin{exercise}
  What is a great practical problem with organizing your code (with
  the emphasis on `code'!) this way?
\end{exercise}

If the number of points per processor is large enough, the amount of
communication is low relative to the computation, and you could take
$b$ fairly large. However, these grid updates are mostly used in
iterative methods such as the \indexac{CG} method
(section~\ref{sec:cg}), and in that case considerations of roundoff
prevent you from taking $b$ too large\cite{ChGe:sstep}.

\begin{exercise}
  Go through the complexity analysis for the non-overlapping algorithm
  in case the points are organized in a 2D grid. Assume that each
  point update involves four neighbours, two in each coordinate
  direction.
\end{exercise}

A further refinement of the above algorithm is possible.
Figure~\ref{fig:grid-update-minimal} illustrates that it is possible
to use a halo region that uses different points from different time steps.
\begin{figure}[ht]
\includegraphics[scale=.1]{graphics-public/grid-update-minimal}
\caption{Computation of blocks of grid points over multiple iterations}
\label{fig:grid-update-minimal}
\end{figure}
This algorithm (see~\cite{Demmel2008IEEE:avoiding}) cuts down on the amount
of redundant computation. However, now the halo values that are communicated
first need to be computed, so this requires splitting the local communication
into two phases.

\Level 0 {Block algorithms on multicore architectures}
\label{sec:multicore-block}
\input chapters-public/ernie

